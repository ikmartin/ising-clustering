\documentclass{article}
\usepackage{geometry}[1in]
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\softmax}{soft\,max}

\title{Quadratic Rigidity}
\author{Andrew Moore}
\date{Summer 2023}
\begin{document}
\maketitle

The goal is to figure out what number and distribution of constraints we need on a given input level in order to get at least a certain probability that the entire constraint matrix will be satisfied (or really, satisfiable, but that is a harder problem it seems). 

One way to attack this is to take advantage of the rigidity of the quadratic function class. Suppose that $H$ is a quadratic polynomial in $x_1 \dots x_n$, where we have coefficients $c_{ij} \sim N(0,1)$. 

\section{Building}

Suppose that we have two quadratic Hamiltonians $H_1(x, o_1, a_1)$ and $H_2(x, o_1, o_2, a_2)$ such that if $M = 2\max |H_2|+1$, then
\begin{align}
	H_1(x, o_1, a_1) <= H_1(x, o_1', a_1') - M &&\forall (o_1', a_1') \neq (o_1, a_1)\\
	H_2(x,o_1,o_2, a_2) <= H_2(x,o_1,o_2',a_2')-1 &&\forall (o_2', a_2') \neq (o_2, a_2)
\end{align}
Then we can make a new Hamiltonian $H(x,o_1,o_2,a_1,a_2) = H_1(x,o_1,a_1) + H_2(x,o_1,o_2,a_2)$. Select wrong outputs $(o_1', a_1') \neq (o_1,a_1)$ and $(o_2', a_2') \neq (o_2, a_2)$.
\begin{align}
	H(x,o_1',o_2',a_1',a_2') = H_1(x,o_1',a_1') + H_2(x,o_1',o_2',a_2') \\
	\geq H_1(x,o_1,a_1) + M + H_2(x,o_1,o_2',a_2') + (H_2(x,o_1', o_2', a_2') - H_2(x,o_1,o_2',a_2'))
\end{align}
Note that $H_2(x,o_1', o_2', a_2') - H_2(x,o_1,o_2',a_2') \geq -2\max|H_2|$, so we can continue:
\begin{align}
	\geq H_1(x,o_1,a_1) + 1 + H_2(x,o_1,o_2',a_2') \geq H_1(x,o_1,a_1) + H_2(x,o_1,o_2,a_2) + 2\\
	= H(x,o_1,o_2,a_1,a_2) + 2
\end{align}
Similarly, we can cover the other two cases:
\begin{align}
	H(x,o_1,o_2',a_1,a_2') = H_1(x,o_1,a_1) + H_2(x,o_1,o_2',a_2')\\
	\geq H_1(x,o_1,a_1) + H_2(x,o_1,o_2,a_2) + 1 = H(x,o_1,o_2,a_1,a_2) + 1\\
	H(x,o_1',o_2,a_1',a_2) = H(x,o_1',a_1') + H_2(x,o_1',o_2,a_2)\\
	\geq H_1(x,o_1,a_1) + M + H_2(x,o_1,o_2,a_2) + (H_2(x,o_1',o_2,a_2) - H_2(x,o_1,o_2,a_2))\\
	\geq  H_1(x,o_1,a_1) + H_2(x,o_1,o_2,a_2) + 1 = H(x,o_1,o_2,a_1,a_2) + 1
\end{align}
Thus $H$ satisfies this (stronger) constraint set. We can refine $M$ to the better value of 
\begin{align}
	1 + \max\left\{0, -\min_{x,o_1', o_2', a_2'} H_2(x,o_1',o_2',a_2') - H_2(x,o_1,o_2',a_2')\right\}
\end{align}

\section{SVM}

Fitting a degree $d$ Ising Hamiltonian with one output spin and no input-input connections is identical to fitting a $d-1$-degree polynomial kernel SVM. Let $H(x, y)$ be a degree $d$ polynomial satisfying a circuit constraint set $H(x, -f(x)) \geq H(x, f(x)) + 1$. Then since $H(x,y) = yG(x)$ for some degree $d-1$ polynomial $G$, we have $-y_i G(x_i) \geq y_i G(x_i) + 1$, so $y_i G(x_i) \leq -1/2$. Notice that for $G' := -2G$, we have $y_i G'(x_i) \geq 1$, which is the constraint equation for a $d-1$ polynomial kernel SVM. In particular, a feasible quadratic Hamiltonian $yf(x)$ for $f(x) = \langle w, x \rangle + b$ is equivalent to the SVM solution $y_i(\langle -2w, x_i \rangle - 2b) \geq 1$.

We have a number of ways that we could guess a new aux by picking a separating hyperplane. First, let's consider the case of a single output bit. Then there is only one constraint equation, and we're literally dealing with an SVM. Attempting to fit a quadratic will result in some misclassified points. Now we could do one of two things: either take the best-guess hyperplane and set it as the new aux, or fit a best-guess hyperplane which attempts to separate the classified and misclassified points. When thinking about it geometrically, the former allows a sort of hyperbola, while the latter allows a sort of curve-off or cubic looking separating curve (if it is possible). It seems like which one is better depends on the situation, though it's possible that restricting the points to hypercube corners will change the situation somewhat.

One observation we can make is that the $\zeta$ variables in the soft SVM are identifiable with the $\rho$ variables in the aritifial Ising objective. 

\subsection{Multiple Outputs}

Now, consider the case of two outputs. Suppose that $H(x,y,z) = y(\langle u, x\rangle + b) + z(\langle v, x\rangle + c) + dzy$. 
\begin{align}
	H(x,y,z) < H(x,-y,z), H(x,y,-z), H(x,-y,-z) &&\forall x
\end{align}
Expands as 
\begin{align}
	y(\langle u, x\rangle + b) + z(\langle v, x\rangle + c) + dzy &< -y(\langle u, x\rangle + b) + z(\langle v, x\rangle + c) - dzy\\
	y(\langle u, x\rangle + b) + z(\langle v, x\rangle + c) + dzy &< y(\langle u, x\rangle + b) - z(\langle v, x\rangle + c) - dzy\\
	y(\langle u, x\rangle + b) + z(\langle v, x\rangle + c) + dzy &< -y(\langle u, x\rangle + b) - z(\langle v, x\rangle + c) + dzy
\end{align}
Thus
\begin{align}
	yf(x) + dzy < 0\\
	zg(x) + dzy < 0\\
	yf(x) + zg(x) < 0
\end{align}

Now the situation gets much more complicated when we have multiple outputs. Since they are not independent, there is the quadratic remainder, parts of which are added to each constraint equation (as seen above). The lower the basin number, the more of the quadratic remainder is used to perturb the decision boundary, with the highest basin having no perturbation. Let us note that each one of these equations will have an associated $\rho$ value when we attempt to solve. It seems to stand to reason that these are the equations that `need help', so perhaps adding auxilliaries based off the hyperplanes involved would be sensible. The idea is that these equations need more degrees of freedom. Or, maybe we could try to use FME to remove the output-output interactions, and then deal with the resulting (much larger) set of constraint equations on the inputs alone. 

The hypercube constraints version is 
\begin{align}
	yf(x) + zg(x) + dzy < -yf(x) + zg(x) - dzy\\
	yf(x) + zg(x) + dzy < yf(x) - zg(x) - dzy\\
	-yf(x) + zg(x) - dzy < -yf(x) - zg(x) + dzy\\
	yf(x) - zg(x) - dzy < -yf(x) - zg(x) + dzy
\end{align}
Which simplifies to
\begin{align}
	yf(x) + dzy < 0\\
	zg(x) + dzy < 0\\
	zg(x) - dzy < 0\\
	yf(x) - dzy < 0
\end{align}
Which is in fact essentially the same thing as the bit-separate case. Actually, if you think about it, bit-separate and hypercube are the same thing, since bit-separate is automatically hypercube, and if we have hypercube, then flipping a bit from wrong to right always decreases the energy, so we could just set all the other bits arbitrarily fixed and obtain a bit-separate solution.


Consider input space $\Sigma_N$. For each input configuration $\sigma \in \Sigma_N$, we have a space of considered outputs $\Sigma_M(\sigma)$. The desired function is $f(\sigma) \in \Sigma_M(\sigma)$, so it is useful to have the notation $W(\sigma) = \Sigma_M(\sigma) \setminus \{f(\sigma)\}$. Now, assume that there exists some function $F : \Gamma(\Sigma_M) \longrightarrow B^A$ such that there exist Hamiltonians $G$ and $H$ satisfying
\begin{align}
	G(\sigma, \eta, \tau) \geq G(\sigma, \eta, F(\sigma, \eta)) + K_0 &&\forall \sigma \in \Sigma_N, \eta \in \Sigma_M(\sigma), \tau \in B^A\\
	H(\sigma, \eta, F(\sigma, \eta)) \geq H(\sigma, f(\sigma), F(\sigma, f(\sigma))) + K_1 && \forall \sigma \in \Sigma_N, \eta \in W(\sigma)
\end{align}
Define $R = G + H$. Then for any $\sigma \in \Sigma_N, \eta \in \Sigma_M(\sigma), \tau \in B^A$, 
\begin{align}
	R(\sigma, \eta, \tau) = H(\sigma, \eta, \tau) + G(\sigma, \eta, \tau) \geq H(\sigma, \eta, \tau) + G(\sigma, \eta, F(\sigma, \eta)) + K_0\\
	= H(\sigma, \eta, F(\sigma, \eta)) + (H(\sigma, \eta, \tau) - H(\sigma, \eta, F(\sigma, \eta))) + G(\sigma, \eta, F(\sigma, \eta)) + K_0\\
	\geq H(\sigma, f(\sigma), F(\sigma, f(\sigma))) + K_1 + (H(\sigma, \eta, \tau) - H(\sigma, \eta, F(\sigma, \eta))) + G(\sigma, \eta, F(\sigma, \eta)) + K_0\\
	= R(\sigma, f(\sigma), F(\sigma, f(\sigma))) + K_0 + K_1\\
	+(H(\sigma, \eta, \tau) - H(\sigma, \eta, F(\sigma, \eta))) + (G(\sigma, \eta, F(\sigma, \eta)) - G(\sigma, f(\sigma), F(\sigma, f(\sigma))))
\end{align}
Therefore, $R$ is a viable Hamiltonian if
\begin{align}
	(H(\sigma, \eta, \tau) - H(\sigma, \eta, F(\sigma, \eta))) + (G(\sigma, \eta, F(\sigma, \eta)) - G(\sigma, f(\sigma), F(\sigma, f(\sigma)))) + K_0 + K_1 > 0
\end{align}

\section{Entropy}

I have a hypothesis about which threshold functions are better than others. A threshold function based on a set of inputs and outputs should be trying to give us information about whether or not the pattern is correct. One based on only inputs should be trying to tell us information about what the answer is. It is rather annoying that these two interpretations are so different. They are also likely wrong, and it would be a bad idea to assume we know what the aux spin is meant to do, in absence of an actually compelling theory. Therefore, it is better to go with an abstract measure of interpretation.

Consider the set of all I/O patterns $P$, and call the set of correct patterns $R$ and the wrong patterns $W$. Consider a family of binary threshold functions $\hat{f} : P \rightarrow \{0,1\}^a$. Consider a probability density function $p : P \rightarrow [0,1]$, which is in practice given by the input-level-relative Boltzmann probabilities from a best-guess Hamiltonian. There is a true hidden state $x \in P$, and it is up to us to make a guess $\hat{f}$, which will yield a result pattern $\hat{f}(x)$. There are exactly $2^a$ possible result patterns. If we see a possible result pattern $y \in \{0,1\}^a$, then we know that there are $|\hat{f}^{-1}(y)|$ possible true states which could yield that outcome. Therefore,
\begin{align}
	\mathbb{P}(y) = \sum_{z \in \hat{f}^{-1}(y)} p(z) 
\end{align}
It follows that the amount of information given by $y$ is
\begin{align}
	I(y) = -\log_2\sum_{z \in \hat{f}^{-1}(y)} p(z)
\end{align}
Therefore, the expected amount of information given by $\hat{f}$ is 
\begin{align}
	I(\hat{f}) = -\sum_{y \in \{0,1\}^a} \left(\sum_{z \in \hat{f}^{-1} (y)} p(z)\right) \log_2 \sum_{z \in \hat{f}^{-1}(y)} p(z)
\end{align}


\end{document}
