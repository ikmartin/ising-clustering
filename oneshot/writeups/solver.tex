\documentclass{article}
\usepackage{geometry}[1in]
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\softmax}{soft\,max}

\title{Solver Documentation}
\author{Andrew Moore}
\date{Summer 2023}
\begin{document}
\maketitle

\section{Problem Setup}

Our goal is to solve the linear programming problem $M\phi \geq v$, where in general $v$ is the vector of all ones. In practice we can reformulate the problem as 
\begin{align*}
		\min_{\phi, \rho}\ \langle 1, \rho\rangle &\text{ s.t. } M\phi + \rho \geq v, \rho \geq 0
\end{align*}
Note that this is equivalent to
\begin{align*}
		\max_{\lambda, s}\ \langle b, \lambda \rangle &\text{ s.t. } A^T \lambda + s = c, s \geq 0
\end{align*}
If we make the identification
\begin{align*}
		&b = \begin{bmatrix}
				0\\
				-1\end{bmatrix} 
		&c = \begin{bmatrix}
				-v\\
				0\end{bmatrix}
		&&A^T = \begin{bmatrix}
				-M & -I\\
				0  & -I
		\end{bmatrix}
\end{align*}
Since if $\lambda = (\lambda_1, \lambda_2)$ and $s = (s_1, s_2)$, then $A^T\lambda + s = c$ can be written as the pair of equations $-M\lambda_1 - \lambda_2 + s_1 = -v$ and $-\lambda_2 + s_2 = 0$, which can be re-arranged as $s_2 = \lambda_2$ and $s_1 = M\lambda_1 + \lambda_2 - v$, so $s \geq 0$ actually means $\lambda_2 \geq 0$ and $M\lambda_1 + \lambda_2 \geq v$. This recovers our original problem with the identification $\phi := \lambda_1$, $\rho := \lambda_2$. A word on the dimensions. Suppose that $M \in \R^{m \times n}$, with $m >> n$. Then $A^T \in \R^{2m \times n+m}$, so $\lambda, b \in \R^{n+m}$ and $s, c, x \in \R^{2m}$. We can also go ahead and set $r_b \leftarrow Ax - b$ and $r_c \leftarrow A^T\lambda + s - c$. 

\section{Solving Systems of the Form $(AKA^T)p=q$}

Let $A$ be as defined above and $K$ be some general diagonal matrix with diagonal vector $k$ split into $k_1, k_2 \in \R^m$. The only actual systems of equations that we need to solve in this algorithm will be of the form $(AKA^T)p = q$. We will derive a general algorithm for solving this system in an efficient manner by taking advantage of the structure of $A$. Note that $p,q \in \R^{n+m}$.
\begin{align}
		AKA^T = 
		\begin{bmatrix}
				-M^T & 0\\
				-I & -I
		\end{bmatrix}
		\begin{bmatrix}
				K_1 & 0\\
				0 & K_2
		\end{bmatrix}
		\begin{bmatrix}
				-M & -I\\
				0 & -I
		\end{bmatrix}\\
		= 
		\begin{bmatrix}
				-M^T & 0\\
				-I & -I
		\end{bmatrix}
		\begin{bmatrix}
				-K_1M & -K_1\\
				0 & -K_2
		\end{bmatrix}
		= 
		\begin{bmatrix}
				M^TK_1M & M^TK_1\\
				K_1M & K_1 + K_2
		\end{bmatrix}
\end{align}
Now, we can apply block UDL-decomposition. It will be convenient if the matrix that we have to invert is actually the bottom right (since it is diagonal), and therefore we need the formula 
\begin{align}
		\begin{bmatrix}
				W & X\\
				Y & Z
		\end{bmatrix}
		=
		\begin{bmatrix}
				I & XZ^{-1}\\
				0 & I
		\end{bmatrix}
		\begin{bmatrix}
				W - XZ^{-1}Y & 0\\
				0 & Z
		\end{bmatrix}
		\begin{bmatrix}
				I & 0\\
				Z^{-1}Y & I
		\end{bmatrix}
\end{align}
Applying this, we obtain (with $D := K_1$ and $Z := K_1 + K_2$)
\begin{align}
		\begin{bmatrix}
				I & M^T DZ^{-1}\\
				0 & I
		\end{bmatrix}
		\begin{bmatrix}
				M^T(D - D^2Z^{-1})M & 0\\
				0 & Z
		\end{bmatrix}
		\begin{bmatrix}
				I & 0\\
				DZ^{-1} M & I
		\end{bmatrix}\\
		= 
		\begin{bmatrix}
				I & M^T DZ^{-1}\\
				0 & I
		\end{bmatrix}
		\begin{bmatrix}
			M^T(D-D^2Z^{-1})M & 0\\
			DM & Z
		\end{bmatrix}
\end{align}
Now, consider the equation
\begin{align}
		\begin{bmatrix}
				I & M^T DZ^{-1}\\
				0 & I
		\end{bmatrix}
		\begin{bmatrix}
			y_1\\
			y_2
		\end{bmatrix}
		=
		\begin{bmatrix}
			q_1\\
			q_2
		\end{bmatrix}
\end{align}
We get $y_2 = q_2$ and $q_1 = y_1 + M^TDZ^{-1} y_2 = y_1 + M^TDZ^{-1} q_2$, so $y_1 = q_1 - M^TDZ^{-1}q_2$. Now, we want to solve
\begin{align}
		\begin{bmatrix}
			M^T(D-D^2Z^{-1})M & 0\\
			DM & Z
		\end{bmatrix}
		\begin{bmatrix}
			p_1\\
			p_2
		\end{bmatrix}
		=
		\begin{bmatrix}
			q_1 - M^TDZ^{-1}q_2\\
			q_2
		\end{bmatrix}
\end{align}
Clearly $DM p_1 + Zp_2 = q_2$ implies $p_2 = Z^{-1}(q_2 - DMp_1)$, so we have reduced the problem to solving the linear system $(M^T(D-D^2Z^{-1})M)p_1 = q_1 - M^TDZ^{-1}q_2$. Since $n$ is in general fairly small, this is actually an easy system to solve. This leads us to the following algorithm:
\begin{align}
	p_1 &\leftarrow (M^T(D-D^2Z^{-1})M)^{-1}(q_1 - M^TDZ^{-1}q_2)\\
	p_2 &\leftarrow Z^{-1}(q_2 - DMp_1)
\end{align}

\section{Solving the Main System}

We need to solve systems of the form
\begin{align}
		\begin{bmatrix}
				0 & A^T & I\\
				A & 0   & 0\\
				S & 0   & X
		\end{bmatrix}
		\begin{bmatrix}
				\Delta x\\
				\Delta \lambda\\
				\Delta s
		\end{bmatrix} 
		= 
		\begin{bmatrix}
				-r_c\\
				-r_b\\
				L
		\end{bmatrix}
\end{align}
Which, written out as equations, is
\begin{align}
		A^T\Delta \lambda + \Delta s = -r_c\\
		S\Delta x + X\Delta s = L\\
		A\Delta x = -r_b
\end{align}
We can re-arrange to determine that $\Delta s = -r_c - A^T\Delta \lambda$ and $\Delta x = S^{-1}(L - X\Delta s) = S^{-1}(L - X(-r_c - A^T\Delta \lambda))$, upon which the last equation becomes
\begin{align}
		AS^{-1}(L - X(-r_c - A^T\Delta \lambda)) = -r_b\\
		AS^{-1}L + AS^{-1}Xr_c + AS^{-1}XA^T\Delta \lambda = -r_b\\
		(AS^{-1}XA^T) \Delta \lambda = -r_b - AS^{-1}(L + Xr_c)\\
		(AS^{-1}XA^T) \Delta \lambda = -Ax + b - AS^{-1}(L + Xr_c)\\
		= b - A(x + S^{-1}(L + Xr_c))
\end{align}
This gives us the following algorithm:
\begin{align}
	\Delta \lambda &\leftarrow (AS^{-1}XA^T)^{-1}(b-A(x + S^{-1}(Xr_c + L)))\\
	\Delta s &\leftarrow -r_c - A^T\Delta \lambda\\
	\Delta x &\leftarrow S^{-1}(L - X\Delta s)
\end{align}

\section{Optimizations}

Since the constraint matrix $M$ is in practice about half composed of zeroes (for quadratic problems---the sparsity is much higher for higher degree polynomial fitting), the first natural optimization is to store $M$ in a sparse matrix format. My solver chooses to use CSC (Compressed Sparse Column) format, because it is both a widly used standard and amenable to the problem. Profiling an implementation of the algorithm in C shows that the most expensive step by far is the calculation of the coefficient matrix for the system of equations. Since it always has the structure $M^TDM$ for some diagonal matrix $D$, this comes down to calculating $n^2$ weighted column-column inner products of $M$, so compressing the columns is certainly the most natural sparse representation. We can first observe that since this matrix is symmetric, only the upper triangle needs to be computed, which cuts the cost in half. Now, if $C_i$ is the $i$th column of $M$, we need a quick way to calculate $\langle C_i, dC_j\rangle$ for a coefficient $d$ which is not known ahead of time. Since the two columns are both sparse, and known ahead of time, I have chosen to precompute for each pair of columns $C_i, C_j$ a list $R_{ij}$ of the indices at which they are simultaneously nonzero, as well as a vector $S_{ij}$ of their products such that $S_{ij}(k) = (C_i)_{R_{ij}(k)}(C_j)_{R_{ij}(k)}$. That way, we can compute only exactly what really needs to be done, that is $\sum_k S_{ij}(k)d_{R_{ij}(k)}$. 

The next optimization to make is to notice that due to the structure of the constraint matrix, there may be substantial strings of such weighted sums and differences of the elements of $d$ which appear in many different column-column products. Precomputing these could in principle save a lot of repeated effort. However, because (unlike in the case of Teresa's solver) the constraint matrices $M$ do not follow any exact format, there does not appear to be any one single optimal way of doing this. An additional consideration is that it seems more obvious how to multithread without doing this, since because computing any common values, we may well consider the generation of the coefficient matrix an embarressingly parallel problem on the number of upper-triangular matrix entries. 

To do the multithreading, I chose to hand-roll a threadpool. The threads are always running, and when given a wakeup broadcast will check a status flag to see what task they have to do. Then they will draw from a `queue' (really a precomputed list) of task indices. When all tasks are complete, the calling thread is signalled. It is quite obvious how to multithread the coefficient matrix generation and the $M^T$ multiplication, but the $M$ multiplication is not as convenient, since CSC does not lend itself to parallelizing such operations. One solution might be a large array of mutexes for each element in the result, but this seems a little inefficient. 

\end{document}
