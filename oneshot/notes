2x2x1
2x3x2
2x4x3
2x5x4
2x6x5

3x3x4

#340 1
#341 4
#342 8?
#343 9?
#344 4
#345 2
#346 0

270 0 
271 1
272 4?
273 4?
274 4?
275 3
276 2
277 2
278 0



4x4(0)x1
4x4(1)x5


Next ideas:
- Randomly sample the 'wrong answer' constraints. the idea is that the rigidity of the energy function should make neighboring wrong answer constraints redundant. Therefore a random sample may be enough to disqualify an aux array with a small fraction of the constraint set. Additionally, we may want to consider increasing the density of the sampling when closer to the right answer, on the logic that being closer to the right answer makes higher 'resolution' more important. 
- A similar idea which could be added on: we could again take advantage of the rigidity of the energy function class to try to compensate for the resolution lost by random sampling of the constraints. We could randomly sample the constraints in a sparse manner, but require that the difference in Hamiltonian between the right answer and a far-away wrong answer be larger depending on that distance--in other words, a combination of hypercube constraints and sparse constraint sampling, which may balance each other out in terms of strictness. The downside is that this may be capable of producing false negatives. However, given that we want to enforce higher boltzmann probabilities at some point anyway, that might not be the worst thing in the world. 

- Obviously, allowing fancier reduction methods than Rosenberg when constructing the aux arrays. This is a bit of a pain to program but it seems clear that this could only improve the situation. 
- Of course, we could also just allow the creation of aux arrays via random other small polynomials of the variables. This seems like a good addition of flexibility, but it does introduce the problem that we need to know how to track progress without doing a refit of the entire situation as was the initial method. The fact that the reductions come from poly-reduction methods allows use to use the continually reducing polynomial as a progress tracker.

- We know that we need a better heuristic to guide the choices of which pairs to prioritize. One idea is to use the 'sum-of-rho' method that is standard among the rest of the team. The problem is that this appears to be mostly incompatible with constraint filtering, since the quality of the heuristic depends on being fit to the entire constraint set, or at least close to it, and extrapolating from a small subset may not be sufficient to give good information.

- We know that some constraints are more important than others. It seems like the problem might be dual-sparse. Are these related? How can we take advantage of the latter fact?

- Easy one: include a lower-bound. Don't bother testing quadratic feasibility if we're below the lower bound. This would clearly give a small speedup, but since the cost goes exponentially with the length of aux array in general, it should be just a small speedup.

- a brute-force compsci approach: we know that adding more solver processes does not seem to help the situation and in fact slows dwn the algorithm. This is probably because of competition for memory access. This means that we may be able to fully take advantage of multiprocessing if we use message-passing to put one or two solvers on each node of Kruskal. Really the only thing that needs to be shared is the dibs cache indicating which tasks have already been taken over. there might be some sublety to this but I think that should be find. MPI is a pain in the butt of course. Maybe Nemo has better parallel memory access?

- I have been thinking somewhat that the higher-degree polynomial fit is something like a taylor expansion. We know from observation that l1-regularized hamiltonians often tend to have small coefficients on the higher order terms, so it seems like each degree is a sucessively smaller correction of the previous approximation. To this end, we may be able to think of the higher order terms as less important in some sense: what if we fit a quite high-degree polynomial, and then focus only on Rosenberg reduction of the cubic terms? Will it turn out that those higher-order terms were less important than appeared at first? They do activate much less often, being multi-op AND gates. Furthermore, it seems suggestive that the cubics which do make it in to the higher-degree polynomial fit may be significantly more important than your average cubic from the cubic Hamiltonian. Of course a sparser high-degree hamiltonian does provide the opportunity for logarithmic preformance out of FGBZ, but I had already thought of that---unfortunately, while it is of course worth trying (I am going to add FGBZ options anyway), I suspect that we may not get the expected benefits, as manual inspection of the higher degree hamiltonians indicates that they do not actually have a huge amount of high-degree common factors. 

- It seems to stand to reason that we could get our LP solvers to work more quickly if they were instructed only to figure out if as solution was feasible, rather than actually producing it. For one thing, l1 regularization requires double the number of variables, and twice as many more constraints.This suggests that we could remove the objective and obtain solutions faster. On the other hand, my attempt at trying this mysteriously did not work. That is a somewhat disquieting error, which should be looked into. 

- Delegator slowdown is starting to become signficant. Of course there isn't really much of a problem with just adding more delegator processes, but still it may be worth looking into optimizing the polynomial code at some point. it isn't clear that storing everything as a dict is really the right solution, as we end up just iterating over items() quite a lot anyway. 

- Insofar as what we are currently doing is just trying out a bunch of different reduction paths and continually attempting quadratic fit at each node in the tree---perhaps it would be worth attempting (and it is far cheaper) to actually find that optimal Rosenberg reduction and so on before going through all this. What I have in mind is a two part algorithm: first, work through the NP-hard problem of finding the optimal reduction, presumably by enumerating out the big reduction tree with all the methods and so on. We can prune this by ignoring obviously bad branches for an optimization, but for now in theory just imagine that we have the entire tree. Then we could iterate through the tree with quadratic feasibility checking. The big advantage is that if we have the tree enumerated out down to the leaves, then the final auxilliary numbers of the leaves provides us a much better heuristic than we are currently using, though based on the same method, for we don't have to rely on any heuristics to know the optimal rosenberg cost of a given reduction option---it just becomes a minimax problem. With the heuristic values precomputed, we may be able to to much better in terms of picking the right tasks during the actualy solvability-search stage (as we know, since good solutions sometimes appear after a while of searching, the current heuristics are pretty bad, although they do point us in the right overall direction). This seems like a natural thing to try, given that we have already abandoned as too expensive (and seemingly not worth it) the idea of refitting the higher degree hamiltonian at each step in order to obtain new reduction candidates. As Jess mentioned, we could of course still just do this sometimes, but the fact that the results of the two methods are so far equivalent indicates to me that the extra effort is simply not worth it. Finding the best possible Rosenberg reduction is curiously something that we have not actually attempted at all so far, even for the smallest problems where it is clearly computationally feasible---I suppose the NP-hard label just scared us off. 

- In general, we need better heuristics to guide solver priority, and we need better and quicker ways to determine if a quadratic problem is infeasible. However, we have had very good results with this so far, and we're definitely still making progress. I have hope!


3x4 results from Kruskal (single run)
[^[[33;20mSolver-3^[[0m] Found new best aux array with length 9^[[0m
[^[[33;20mSolver-3^[[0m] History: [((0, 9), 'ros'), ((4, 11), 'ros'), ((1, 10), 'ros'), ((3, 11), 'ros'), ((10, 11), 'ros'), ((8, 10), 'ros'), ((5, 11), 'ros'), ((1, 12), 'ros'), ((3, 10), 'ros')]^[[0m

[^[[33;20mSolver-4^[[0m] Found new best aux array with length 8^[[0m
[^[[33;20mSolver-4^[[0m] History: [((0, 9), 'ros'), ((4, 11), 'ros'), ((1, 10), 'ros'), ((3, 11), 'ros'), ((8, 10), 'ros'), ((5, 11), 'ros'), ((1, 12), 'ros'), ((3, 10), 'ros')

[^[[33;20mSolver-5^[[0m] Found new best aux array with length 7^[[0m
[^[[33;20mSolver-5^[[0m] History: [((0, 9), 'ros'), ((4, 11), 'ros'), ((1, 10), 'ros'), ((0, 11), 'ros'), ((5, 12), 'ros'), ((0, 10), 'ros'), ((1, 2), 'ros')



3x4x7

original variables
 0  1  2  3  4  5  6    7  8  9 10 11 12 13
x0 x1 x2 y0 y1 y2 y3 | o0 o1 o2 o3 o4 o5 o6

Instructions: Run with mask=[0,1,2,5] include=[6] (24.58 seconds)
       0  1  2  3  4  5  6  7    8  9 10 11
vars: x0 x1 x2 y0 y1 y2 y3 o6 | o0 o1 o2 o5
Result: (0,10) (5,11) (1,7)     (length 2 is impossible)
-> x0o2 y2o5 x1o6

now, include these aux in the next stage.

run with mask=(3,4) include = (0,1,2,5,6) and_pairs = ((0,9), (5,12), (1,13))

	     0  1  2  3  4  5  6  7  8  9 10 11   12   13   14   15 16
vars: x0 x1 x2 y0 y1 y2 y3 o0 o1 o2 o5 o6 x0o2 y2o5 x1o6 | o3 o4
Result: (1,16) (0, 16) (4,15) (3,15)
-> x1o4 x0o4 y1o3 y0o3

aux keys (0,9), (5,12), (1,13), (1,11), (0,11), (4,10), (3,10)

Confirmed by Teresa's solver with verify.py


4x4x13

original vars

 0  1  2  3  4  5  6  7    8  9 10 11 12 13 14 15
x0 x1 x2 x3 y0 y1 y2 y3 | o0 o1 o2 o3 o4 o5 o6 o7

desired (0,6) included (7,)
 0  1  2  3  4  5  6  7  8    9 10
x0 x1 x2 x3 y0 y1 y2 y3 o7 | o0 o6
result: (2,10) (6,10) (5,6)
-> x2o6 y2o6 y1y2

desired (1,5) included (0,6,7) and_pairs (2,14) (6,14) (5,6)
 0  1  2  3  4  5  6  7  8  9 10   11   12   13   14 15
x0 x1 x2 x3 y0 y1 y2 y3 o0 o6 o7 x2o6 y2o6 y1y2 | o1 o5
result: (1,15) (5,15) (13,14) (3,15)
-> x1o5 y1o5 y1y2o1 x3o5

desired (2,3,4) included (0,1,5,6,7) and_pairs (2,14) (6,14) (5,6) (1,13) (5,13) (5,6,9) (3,13)
 0  1  2  3  4  5  6  7  8  9 10 11 12   13   14   15   16   17     18   19   20 21 22
x0 x1 x2 x3 y0 y1 y2 y3 o0 o1 o5 o6 o7 x2o6 y2o6 y1y2 x1o5 y1o5 y1y2o1 x3o5 | o2 o3 o4
result: (4,20) (4,21) (0,21) (4,22) (20,22) (0,22)
-> y0o2 y0o3 x0o3 y0o4 o2o4 x0o4

Complete: x2o6 y2o6 y1y2 x1o5 y1o5 y1y2o1 x3o5 y0o2 y0o3 x0o3 y0o4 o2o4 x0o4


4x4x13

desired (0,6) included (7,)
['x₂o₆', 'y₂o₆', 'y₁o₀'] [(2, 14), (6, 14), (5, 8)

desired (1,5) included (0,6,7)
['x₁o₅', 'y₁o₅', 'x₃y₂', 'x₀o₁'] [(1, 13), (5, 13), (3, 6), (0, 9)]

desired (2,3,4) included (0,1,5,6,7)
['x₀o₂', 'y₀o₃', 'x₁o₄', 'x₀o₄', 'x₂o₄', 'y₁o₃'] [(0, 10), (4, 11), (1, 12), (0, 12), (2, 12), (5, 11)]


(New method: fixed auxes...)
4x4x10


desired (0,1,5,6) incl (7,)
['x₁o₅', 'x₂o₆', 'x₁o₁', 'y₁o₅'] [(1, 13), (2, 14), (1, 9), (5, 13)]

desired (2,3,4) incl (0,1,5,6,7)
['x₀x₁', 'y₀o₃', 'x₂o₂', 'x₀o₃', 'y₁o₄', 'y₀o₄'] [(0, 1), (4, 11), (2, 10), (0, 11), (5, 12), (4, 12)]

4x4x9 (fixed aux)

desired (0,1,2,5,6) incl(7,)
['x₀x₁', 'y₁o₅', 'y₂o₆', 'y₁o₂', 'x₂o₅'] [(0, 1), (5, 13), (6, 14), (5, 10), (2, 13)]

desired (4,5) incl (0,1,2,5,6,7)
['x₀o₃', 'x₁o₃', 'y₀o₄', 'y₁o₄'] [(0, 11), (1, 11), (4, 12), (5, 12)]


4x4x11  (fixed aux)

desired (0,1,2,3,6) incl(7)
['x₀o₁', 'x₁o₃', 'x₀o₂', 'x₂o₃', 'y₂o₆'] [(0, 9), (1, 11), (0, 10), (2, 11), (6, 14)]

['x₀x₁', 'o₄o₅', 'x₂o₅', 'x₂o₄', 'x₃o₄', 'y₁o₅'] [(0, 1), (12, 13), (2, 13), (2, 12), (3, 12), (5, 13)]


4x4x8 (fixed aux)

desired (0,1,2,3,5,6) incl(7)
['x₀x₁', 'y₀o₃', 'x₀o₂', 'x₂o₅', 'x₂o₆', 'y₁o₅'] [(0, 1), (4, 11), (0, 10), (2, 13), (2, 14), (5, 13)]

desired (4,) incl(0,1,2,3,5,6,7)
['y₁o₄', 'y₀o₄'] [(5, 12), (4, 12)]

5x5x33 (free auxes)

desired (0,8) included (9,)
x₃o₈ y₁o₀ y₃o₈ [(3, 18), (6, 10), (8, 18)]

desired (1,) included (0,8,9) 
x₀o₁ o₉o₁ y₂o₁ [(0, 11), (19, 11), (7, 11)

desired (2,) included (0,1,8,9) 
y₃o₂ y₀o₂ x₀o₂ [(8, 12), (5, 12), (0, 12)]

desired (3,) included (0,1,2,8,9) 
y₀o₃ y₁o₃ x₂o₃ x₀o₃ [(5, 13), (6, 13), (2, 13), (0, 13)]

desired (7,) included (0,1,2,3,8,9)
x₂o₇ y₂o₇ x₄y₃ [(2, 17), (7, 17), (4, 8)]

desired (4,) included (0,1,2,3,7,8,9)
['x₀x₁', 'y₁o₄', 'x₁o₄', 'x₃o₄', 'x₄o₄', 'x₂o₄'] [(0, 1), (6, 14), (1, 14), (3, 14), (4, 14), (2, 14)]

desired (5,) included (0,1,2,3,4,7,8,9)
 ['x₀x₂', 'x₁o₅', 'y₁o₅', 'y₂o₅', 'y₃o₅', 'y₀o₅'] [(0, 2), (1, 15), (6, 15), (7, 15), (8, 15), (5, 15)

desired (6,) included (0,1,2,3,4,5,7,8,9)
['x₀o₆', 'x₁o₆', 'x₂o₆', 'x₄o₆', 'x₃o₆'] [(0, 16), (1, 16), (2, 16), (4, 16), (3, 16)]


New ideas:
note that an aux which can be assumed to be fixed is the same as a spin which is solvable with no auxilliaries (we know AND is always feasible for instance). Therefore, the space of auxes which can be assumed to be fixed is identical with the space of linear separations of the subset of hypercube corners given by the current input set. Now we know that a quadratic hamiltonian is the same as a linear SVM is the same as a separating hyperplane (with orientation). Now, note that we can try to solve a single arbitrary desired output bit by soft SVM. This will return a best-guess hyperplane and a misclassification error, which we hypothesize is the same as the rhos (please check!). therefore, we get a new idea for aux array generation: fit a soft SVM and use its separating hyperplane to define a new fixed auxilliary spin. Repeat. 



Isaac pointed out that input-output AND gates cannot be assumed to be fixed, since precomputing them would require that we know the outputs in advance. This explains why my 4x4x8 did not work, since it was based on the faulty assumption that all AND auxilliaries could always be assumed to be fixed inputs. However, there is something similar which IS true--that we can always assume ANDs to be correctly calculated. What do I mean? Well we can just have AND gates fixed, but their output is not fixed. So if we have row (x1,x2,y) = (1,0,0), then x1 AND y = 0, and row (x1, x2, y') = (1,0,1), then x1 AND y = 1. So we can perfectly legitimately have rows (1,0,0,0) and (1,0,1,1), where x3 = x1 AND y. Since AND gates are functional subcircuits, we don't need to include the possibility that the AND calculation is wrong, it's just that the ouput value of the AND gate depends on the output level as well as the input level in this case. So we can still get away with keeping the number of constraint rows constant---it's just unclear whether this will actually still produce the happy results. 

Note that it might be a little hard to see how such a constant function would be made. But it is possible. Suppose that we have a pair x, y, and we want to add a spin a that will always have value x AND y (and can be subsequently used by the circuit). Then we can add P = M(xy - 2xa - 2ya + 3a) to the Hamiltonian, for M sufficiently large. If a = xy, then P = M(xy - 2xxy - 2yxy + 3xy) = 0, and if a != xy, then a = 1-xy, so P = M(xy - 2x(1-xy) - 2y(1-xy) + 3(1-xy)) = M(xy - 2x + 2xy - 2y + 2xy + 3 - 3xy) = M(2xy - 2x - 2y + 3). If x,y=1 then P = 3M, if x=0 y=1 then P = M, and if x=y=0 then M = 3M. Thus we can simply make M large enough that whatever other hamiltonian we have, it will always be favorable that a = xy. This is precisely the Rosenberg penalty term, and doesn't extend in any obvious way to other linearly computable functions of the variables, but it does mean that we can fix AND gates freely. 

desired 0,1,2,3,6 incl 7
['y₂o₆', 'x₂o₂', 'y₂o₃', 'x₀o₂', 'x₂o₃', 'x₀o₁', 'x₁o₃', 'y₁o₃', 'o₇o₆'] [(6, 14), (2, 10), (6, 11), (0, 10), (2, 11), (0, 9), (1, 11), (5, 11), (15, 14)]
or
['x₁o₃', 'y₀o₂', 'x₀o₃', 'x₂o₂', 'y₂o₂', 'y₀o₁', 'o₇o₆', 'x₂o₆'] [(1, 11), (4, 10), (0, 11), (2, 10), (6, 10), (4, 9), (15, 14), (2, 14)]

desired 4,5 incl 0,1,2,3,6,7
['y₁o₅', 'y₃o₅', 'x₁o₅', 'y₁o₄', 'y₂o₄', 'y₀o₄'] [(5, 13), (7, 13), (1, 13), (5, 12), (6, 12), (4, 12)]


3x3 bitwise AND best results (incl 5)
1: 3
2: 4
3: 4
4: 2

SVM assume magic misclassification indicators
1: 2
2: 2
3: 3
4: 2

Magic misclassifier SVM on 4x4:
1, 2, 3, 3, 3, 3, 2






0,1,2,5,6 incl 7: 8
3,4 incl 0,1,2,5,6,7: 



4x4x12 with laptop
((0, 4, 12), 	array([1, 0, 0, 0, 1, 1, 1, 0])), 
((1, 10, 11), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((3, 6, 14), 	array([1, 0, 0, 0, 1, 1, 1, 0])), 
((4, 5, 12), 	array([1, 1, 1, 1, 1, 0, 1, 0])), 
((6, 10, 13), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((0, 10, 13), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((0, 5, 13), 	array([1, 0, 0, 0, 1, 1, 1, 0])), 
((1, 13, 14), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((0, 2, 14), 	array([1, 0, 1, 1, 1, 0, 1, 1])), 
((2, 4, 11), 	array([1, 0, 0, 0, 1, 1, 1, 0])), 
((1, 3, 12), 	array([0, 0, 0, 0, 0, 0, 0, 0])), 		# these last two appear useless...
((8, 9, 13), 	array([1, 1, 1, 1, 0, 0, 0, 0]))]


4x4x10 with laptop
((0, 4, 12), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((4, 5, 11), array([1, 1, 1, 1, 1, 0, 1, 0])), 
((2, 6, 12), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((2, 7, 14), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((1, 4, 13), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((0, 1, 13), array([1, 1, 1, 1, 1, 0, 1, 0])), 
((0, 5, 12), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((6, 9, 12), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((3, 8, 14), array([1, 0, 0, 0, 1, 1, 1, 0])), 
((4, 6, 10), array([1, 0, 0, 0, 1, 1, 1, 0]))]
